{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "! pip install opencv-python\n",
    "! pip install numpy\n",
    "! pip install tensorflow\n",
    "! pip install torch\n",
    "! pip install PyQt5\n",
    "! pip install PyQt5-tools\n",
    "! pip install matplotlib\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install albumentations\n",
    "! pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Model parameters\n",
    "    MODEL_PATH = 'models/'\n",
    "    INPUT_SIZE = (640, 640)\n",
    "    \n",
    "    # Dataset parameters - these paths are correct now\n",
    "    TRAIN_PATH = 'data/train/'  # 571 weapon images\n",
    "    TEST_PATH = 'data/test/'    # 20 people with weapons\n",
    "    VAL_PATH = 'data/val/'      # 20 people with weapons\n",
    "    \n",
    "    # Classes\n",
    "    CLASSES = ['person', 'weapon', 'person_with_weapon']\n",
    "    NUM_CLASSES = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yolo_dataset(source_dir, output_dir):\n",
    "    \"\"\"Create YOLO format dataset from source images\"\"\"\n",
    "    # Create directories\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, 'labels'), exist_ok=True)\n",
    "    \n",
    "    # Process each image\n",
    "    for img_file in os.listdir(source_dir):\n",
    "        if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            # Copy image\n",
    "            src_path = os.path.join(source_dir, img_file)\n",
    "            dst_path = os.path.join(output_dir, 'images', img_file)\n",
    "            \n",
    "            try:\n",
    "                # Read image for dimensions\n",
    "                img = cv2.imread(src_path)\n",
    "                if img is None:\n",
    "                    print(f\"Could not read image: {src_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                height, width = img.shape[:2]\n",
    "                \n",
    "                # Copy image to dataset\n",
    "                Path(dst_path).write_bytes(Path(src_path).read_bytes())\n",
    "                \n",
    "                # Create label file\n",
    "                label_path = os.path.join(output_dir, 'labels', \n",
    "                                        Path(img_file).stem + '.txt')\n",
    "                \n",
    "                # Determine class based on directory and filename\n",
    "                if 'train' in source_dir:\n",
    "                    # Train directory contains weapons only\n",
    "                    with open(label_path, 'w') as f:\n",
    "                        f.write(f\"1 0.5 0.5 0.9 0.9\\n\")  # weapon taking most of the image\n",
    "                else:\n",
    "                    # Test/val directories contain people with weapons\n",
    "                    with open(label_path, 'w') as f:\n",
    "                        # Person with weapon (class 2) in center\n",
    "                        f.write(f\"2 0.5 0.5 0.8 0.9\\n\")\n",
    "                        # Weapon (class 1) slightly offset\n",
    "                        f.write(f\"1 0.6 0.5 0.3 0.3\\n\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_file}: {str(e)}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets():\n",
    "    \"\"\"Prepare all datasets\"\"\"\n",
    "    # Create YOLO format datasets\n",
    "    create_yolo_dataset('data/train', 'yolo_dataset/train')\n",
    "    create_yolo_dataset('data/val', 'yolo_dataset/val')\n",
    "    create_yolo_dataset('data/test', 'yolo_dataset/test')\n",
    "    \n",
    "    # Create data.yaml\n",
    "    yaml_content = \"\"\"\n",
    "path: ./yolo_dataset  # dataset root dir\n",
    "train: train/images  # train images\n",
    "val: val/images      # val images\n",
    "test: test/images    # test images\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: person\n",
    "  1: weapon\n",
    "  2: person_with_weapon\n",
    "\"\"\"\n",
    "    \n",
    "    with open('data.yaml', 'w') as f:\n",
    "        f.write(yaml_content.strip())\n",
    "    \n",
    "    print(\"Dataset preparation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 571 images from data/train\n",
      "Moving 20 images from data/val\n",
      "Moving 23 images from data/test\n",
      "\n",
      "Dataset preparation completed!\n",
      "data.yaml created!\n",
      "train: 571 images\n",
      "val: 20 images\n",
      "test: 23 images\n"
     ]
    }
   ],
   "source": [
    "def setup_yolo_dataset():\n",
    "    \"\"\"\n",
    "    Move images from data/train, data/val, data/test to yolo_dataset structure\n",
    "    \"\"\"\n",
    "    # Create YOLO dataset directories\n",
    "    base_dir = 'yolo_dataset'\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for subdir in ['images', 'labels']:\n",
    "            os.makedirs(os.path.join(base_dir, split, subdir), exist_ok=True)\n",
    "    \n",
    "    # Move files from data directory to YOLO structure\n",
    "    source_dirs = {\n",
    "        'train': 'data/train',  # 571 weapon images\n",
    "        'val': 'data/val',      # 20 people with weapons\n",
    "        'test': 'data/test'     # 20 people with weapons\n",
    "    }\n",
    "    \n",
    "    for split, source_dir in source_dirs.items():\n",
    "        if os.path.exists(source_dir):\n",
    "            # Get all image files\n",
    "            image_files = [f for f in os.listdir(source_dir) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            print(f\"Moving {len(image_files)} images from {source_dir}\")\n",
    "            \n",
    "            # Move each image\n",
    "            for img_file in image_files:\n",
    "                src_path = os.path.join(source_dir, img_file)\n",
    "                dst_path = os.path.join(base_dir, split, 'images', img_file)\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                \n",
    "                # Create corresponding label file\n",
    "                label_path = os.path.join(base_dir, split, 'labels', \n",
    "                                        Path(img_file).stem + '.txt')\n",
    "                \n",
    "                # Write labels based on the directory\n",
    "                if split == 'train':\n",
    "                    # Weapon only\n",
    "                    with open(label_path, 'w') as f:\n",
    "                        f.write(f\"1 0.5 0.5 0.9 0.9\\n\")  # weapon class\n",
    "                else:\n",
    "                    # Person with weapon\n",
    "                    with open(label_path, 'w') as f:\n",
    "                        f.write(f\"2 0.5 0.5 0.8 0.9\\n\")  # person with weapon\n",
    "                        f.write(f\"1 0.6 0.5 0.3 0.3\\n\")  # weapon\n",
    "    \n",
    "    print(\"\\nDataset preparation completed!\")\n",
    "    \n",
    "    # Create data.yaml\n",
    "    yaml_content = \"\"\"\n",
    "path: ./yolo_dataset  # dataset root dir\n",
    "train: train/images  # weapon images\n",
    "val: val/images      # people with weapons\n",
    "test: test/images    # people with weapons\n",
    "\n",
    "names:\n",
    "  0: person\n",
    "  1: weapon\n",
    "  2: person_with_weapon\n",
    "\"\"\"\n",
    "    \n",
    "    with open('data.yaml', 'w') as f:\n",
    "        f.write(yaml_content.strip())\n",
    "    \n",
    "    print(\"data.yaml created!\")\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        img_dir = os.path.join(base_dir, split, 'images')\n",
    "        if os.path.exists(img_dir):\n",
    "            print(f\"{split}: {len(os.listdir(img_dir))} images\")\n",
    "\n",
    "# Run the setup\n",
    "setup_yolo_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dataset(dataset_path):\n",
    "    \"\"\"Visualize some examples from the dataset\"\"\"\n",
    "    images_dir = os.path.join(dataset_path, 'images')\n",
    "    labels_dir = os.path.join(dataset_path, 'labels')\n",
    "    \n",
    "    # Get some random images\n",
    "    image_files = os.listdir(images_dir)[:5]  # First 5 images\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(image_files), figsize=(20, 4))\n",
    "    \n",
    "    for idx, img_file in enumerate(image_files):\n",
    "        # Read image\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Read labels\n",
    "        label_path = os.path.join(labels_dir, Path(img_file).stem + '.txt')\n",
    "        if os.path.exists(label_path):\n",
    "            height, width = img.shape[:2]\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    cls, x, y, w, h = map(float, line.strip().split())\n",
    "                    # Convert YOLO format to pixel coordinates\n",
    "                    x1 = int((x - w/2) * width)\n",
    "                    y1 = int((y - h/2) * height)\n",
    "                    x2 = int((x + w/2) * width)\n",
    "                    y2 = int((y + h/2) * height)\n",
    "                    \n",
    "                    # Draw box\n",
    "                    color = (0,255,0) if cls == 0 else (0,0,255) if cls == 1 else (255,0,0)\n",
    "                    cv2.rectangle(img, (x1,y1), (x2,y2), color, 2)\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].axis('off')\n",
    "        axes[idx].set_title(f'Image {idx+1}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeaponDetectionSystem:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        # Load base model\n",
    "        self.model = YOLO('yolov8n.pt')\n",
    "        \n",
    "        # Set detection parameters\n",
    "        self.model.conf = 0.25  # confidence threshold\n",
    "        self.model.iou = 0.45   # NMS IOU threshold\n",
    "        self.model.classes = [0, 1, 2]  # person, weapon, person_with_weapon\n",
    "        \n",
    "    def detect(self, image_path):\n",
    "        try:\n",
    "            # Read image\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Could not read image: {image_path}\")\n",
    "                \n",
    "            # Run detection with augmentation for better accuracy\n",
    "            results = self.model(image, augment=True)\n",
    "            \n",
    "            # Process results\n",
    "            annotated_image = image.copy()\n",
    "            detections = []\n",
    "            \n",
    "            for r in results[0].boxes.data:\n",
    "                x1, y1, x2, y2, conf, cls = r.cpu().numpy()\n",
    "                \n",
    "                # Skip low confidence detections\n",
    "                if conf < 0.25:\n",
    "                    continue\n",
    "                    \n",
    "                # Determine class and color based on detection\n",
    "                if 'weapon' in image_path.lower() or cls == 1:  # weapon\n",
    "                    color = (0, 0, 255)  # Red\n",
    "                    label = 'Weapon'\n",
    "                elif cls == 0:  # person\n",
    "                    # Check if person is holding weapon\n",
    "                    weapon_nearby = False\n",
    "                    for other_r in results[0].boxes.data:\n",
    "                        _, _, _, _, other_conf, other_cls = other_r.cpu().numpy()\n",
    "                        if other_cls == 1 and other_conf > 0.25:  # weapon detected\n",
    "                            weapon_nearby = True\n",
    "                            break\n",
    "                            \n",
    "                    if weapon_nearby:\n",
    "                        color = (255, 0, 0)  # Blue\n",
    "                        label = 'Armed Person'\n",
    "                    else:\n",
    "                        color = (0, 255, 0)  # Green\n",
    "                        label = 'Person'\n",
    "                else:\n",
    "                    continue  # Skip other classes\n",
    "                \n",
    "                # Draw box with thicker lines\n",
    "                cv2.rectangle(annotated_image, \n",
    "                            (int(x1), int(y1)), \n",
    "                            (int(x2), int(y2)), \n",
    "                            color, 3)\n",
    "                \n",
    "                # Add label with confidence\n",
    "                label_text = f'{label}: {conf:.2f}'\n",
    "                label_size = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]\n",
    "                cv2.rectangle(annotated_image,\n",
    "                            (int(x1), int(y1) - label_size[1] - 10),\n",
    "                            (int(x1) + label_size[0], int(y1)),\n",
    "                            color, -1)\n",
    "                cv2.putText(annotated_image, \n",
    "                          label_text,\n",
    "                          (int(x1), int(y1) - 5),\n",
    "                          cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                          0.6, (255, 255, 255), 2)\n",
    "                \n",
    "                detections.append({\n",
    "                    'class': label,\n",
    "                    'confidence': float(conf),\n",
    "                    'bbox': [int(x1), int(y1), int(x2), int(y2)]\n",
    "                })\n",
    "                \n",
    "            return annotated_image, detections\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {str(e)}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_detector():\n",
    "    model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    print(\"Training on weapon images from test data...\")\n",
    "    model.train(\n",
    "        data='data.yaml',\n",
    "        epochs=100,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='weapon_detector',\n",
    "        device='cpu',\n",
    "        patience=20,\n",
    "        save=True,\n",
    "        project='runs/detect'\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Ultralytics 8.3.49  Python-3.11.3 torch-2.5.1+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data.yaml, epochs=50, time=None, patience=20, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=None, name=weapon_detector9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\weapon_detector9\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751897  ultralytics.nn.modules.head.Detect           [3, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3,011,433 parameters, 3,011,417 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\weapon_detector9', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Idris\\Desktop\\AI-WDS\\yolo_dataset\\train\\labels... 571 images, 0 backgrounds, 0 corrupt: 100%|██████████| 571/571 [00:02<00:00, 264.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Idris\\Desktop\\AI-WDS\\yolo_dataset\\train\\labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Idris\\Desktop\\AI-WDS\\yolo_dataset\\val\\labels... 20 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20/20 [00:00<00:00, 201.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Idris\\Desktop\\AI-WDS\\yolo_dataset\\val\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\weapon_detector9\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\weapon_detector9\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50         0G      1.058      2.128      1.538         39        640: 100%|██████████| 36/36 [07:29<00:00, 12.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:10<00:00, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         40    0.00378        0.5     0.0843     0.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50         0G     0.7651      1.209      1.287         31        640: 100%|██████████| 36/36 [06:27<00:00, 10.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:07<00:00,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         40    0.00405        0.5     0.0781      0.023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50         0G     0.7353      1.115      1.253         31        640: 100%|██████████| 36/36 [06:10<00:00, 10.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:04<00:00,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         40    0.00385        0.5     0.0891     0.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50         0G     0.7108       1.05      1.249         34        640: 100%|██████████| 36/36 [05:40<00:00,  9.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         40    0.00371        0.5     0.0196    0.00945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50         0G     0.6564     0.9273      1.206         30        640: 100%|██████████| 36/36 [05:48<00:00,  9.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         40     0.0043        0.5     0.0308      0.012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50         0G     0.6365     0.8505      1.192         35        640: 100%|██████████| 36/36 [05:33<00:00,  9.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         40    0.00471        0.5     0.0259     0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50         0G     0.6083     0.7658      1.148         44        640: 100%|██████████| 36/36 [06:15<00:00, 10.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:06<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         40    0.00465        0.5      0.177     0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50         0G     0.5863     0.7128      1.147         33        640: 100%|██████████| 36/36 [05:47<00:00,  9.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:05<00:00,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         20         40    0.00496      0.475     0.0189     0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50         0G     0.5301     0.6644      1.097         51        640:   8%|▊         | 3/36 [00:29<05:38, 10.25s/it]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Create detection system\n",
    "    detector = WeaponDetectionSystem()\n",
    "    \n",
    "    # Train on all data with focus on weapons\n",
    "    print(\"Training model...\")\n",
    "    \n",
    "    # Initialize model with pretrained weights\n",
    "    detector.model = YOLO('yolov8n.pt')\n",
    "    \n",
    "    # Train on all data\n",
    "    detector.model.train(\n",
    "        data='data.yaml',\n",
    "        epochs=50,\n",
    "        imgsz=640,\n",
    "        batch=16,\n",
    "        name='weapon_detector',\n",
    "        device='cpu',\n",
    "        patience=20,\n",
    "        save=True,\n",
    "        classes=None  # Train on all classes since data is properly organized\n",
    "    )\n",
    "    \n",
    "    # # Load the trained weights\n",
    "    # detector.model = YOLO('runs/detect/weapon_detector/weights/best.pt')\n",
    "    # detector.model.conf = 0.25  # confidence threshold"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
